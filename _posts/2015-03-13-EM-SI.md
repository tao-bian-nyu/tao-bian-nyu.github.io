---
layout: post
title: "Expectation maximization for system identification"
date: 2015-03-13

---

<p>
In this post, we review some existing results on how to perform system identification via expecttion maximization (EM) technique.

First, let us consider a discrete-time linear system

\begin{equation}
x(t+1)=Ax(t)+w(t),\quad y(t)=Cx(t)+v(t),\label{eq:sys}
\end{equation}

where $w$ and $v$ are zero-mean uncorrelated Gaussian noise. $Cov(w)=Q$, $Cov(v)=R$.
Suppose $A$, $Q$, and $R$ are unknown; and only $y$ is measurable.
For simplicity, we assume $C$ and $x(0)=x_0$ are known constant matrix and vector, respectively.
</p>

<p>

The system identification problem here consists with how could to estimate the unknown terms $A$, $Q$, and $R$ by only using the noisy output $y$.

Note that
<ul>
<li> if $x$ is knonw, $A$, $Q$, and $R$ can be evaluated via linear regression. </li>
<li> if $A$ is available, $x$ can be estimated via Kalman's filter.</li>
</ul>
Unfortunally, neither of the two conditions above is valid. 
</p>

<p>
To formalize our design objective, we first, introduce some concepts:
<ul>
<li> The complete data likelihood: $f(x,y;\theta)$ (i.e., $P(x,y;\theta)$).</li>
<li> The incomplete data log-likelihood: $l(y;\theta)=\log\int f(x,y;\theta)\mu(dx)$.</li>
</ul>
Our objective is to find $\theta$ to maximize $l(y;\theta)$.
</p>

<p>
Define
$$
Q(\theta;\theta')=E_x[\log f(x,y;\theta)|y,\theta']=\int \log f(x,y;\theta)p(x|y;\theta')\mu(dx).
$$
The EM algorithm is an iteration method that can be used to maximize the log-likelihood. Each iteration of the EM algorithm consists with two steps. To be specific, at iteration $k$:
<ul>
<li> E step: evaluate $Q(\theta;\theta_k)$.</li>
<li> M step: $\theta_{k+1}=\arg\max_\theta Q(\theta;\theta_k)$.</li>
</ul>

The log-likelihood is  monotonically increasing in each iteration, and is bounded from above. Hence the algorithm indeed converges.
In particular, under certain conditions, one can prove<sup>[1](#fn1)</sup> that $\theta_k\rightarrow \theta^*=\arg\max_\theta l(y;\theta)$.

</p>

<p>

To perform EM for linear system identification, we first derive the log-likelihood for linear systems.<br>

For system \eqref{eq:sys}, we derive conditional probability:
\begin{align*}
P(x(t)|x(t-1))&=\sqrt{\frac{1}{(2\pi)^n|Q|}}\exp\{-\frac{1}{2}(x(t)-Ax(t-1))^TQ^{-1}(x(t)-Ax(t-1))\},\\
P(y(t)|x(t))&=\sqrt{\frac{1}{(2\pi)^m|R|}}\exp\{-\frac{1}{2}(y(t)-Cx(t))^TR^{-1}(y(t)-Cx(t))\}.
\end{align*}

The complete data likelihood is then given as
\begin{align*}
P(\{x(t)\}_{t=0}^T,\{y(t)\}_{t=0}^T)=P(x(0))\prod_{t=1}^TP(x(t)|x(t-1))\prod_{t=0}^TP(y(t)|x(t)).
\end{align*}
Then, 
\begin{align*}
\log P(\{x(t)\}_{t=0}^T,\{y(t)\}_{t=0}^T)=&-\frac{1}{2}\sum_{t=0}^T\{(y(t)-Cx(t))^TR^{-1}(y(t)-Cx(t))\}-\frac{T+1}{2}\log |R|-\frac{T+1}{2}\log2\pi\\
&-\frac{1}{2}\sum_{t=1}^T\{(x(t)-Ax(t-1))^TQ^{-1}(x(t)-Ax(t-1))\}-\frac{T}{2}\log |Q|-\frac{T}{2}\log2\pi.
\end{align*}

The incomplete data log likelihood: 
\begin{align*}
Q(\theta;\theta')=&E\{\log P(\{x(t)\}_{t=0}^T,\{y(t)\}_{t=0}^T)|\{y(t)\}_{t=0}^T,\theta'\}\\
=&-\frac{1}{2}tr\{R^{-1}\sum_{t=0}^T[(y(t)-C\bar x(t))(y(t)-C\bar x(t))^T+C\bar P(t)C^T]\}-\frac{T+1}{2}\log |R|\\
&-\frac{1}{2}Etr\{Q^{-1}\sum_{t=1}^T[(x(t)-Ax(t-1))(x(t)-Ax(t-1))^T]\}-\frac{T}{2}\log |Q|\\
=&-\frac{1}{2}tr\{R^{-1}\sum_{t=0}^T[(y(t)-C\bar x(t))(y(t)-C\bar x(t))^T+C\bar P(t)C^T]\}-\frac{T+1}{2}\log |R|\\
&-\frac{1}{2}tr\{Q^{-1}(\Phi-\Psi A^T-A\Psi^T+A\Xi A^T)\}-\frac{T}{2}\log |Q|,
\end{align*}
where $\theta=(A,Q,R)$,
\begin{align*}
\bar x(t)&=E(x(t)|\{y(s)\}_{s=0}^T,\theta'), \quad \bar P(t)=Cov(x(t)|\{y(s)\}_{s=0}^T,\theta'),\\
\Phi&=\sum_{t=1}^T(\bar P(t)+\bar x(t)\bar x^T(t)), \quad \Psi=\sum_{t=1}^T(\bar P(t,t-1)+\bar x(t)\bar x^T(t-1)),\\
\Xi&=\sum_{t=1}^T(\bar P(t-1)+\bar x(t-1)\bar x^T(t-1)), \  \bar P(t,t-1)=Cov(x(t),x(t-1)|\{y(s)\}_{s=0}^T,\theta').
\end{align*}


Note that in the equation above, $\bar x(t)$, $\bar P(t)$, and $\bar P(t,t-1)$ are estimations based on the whole history of $\{y(s)\}_{s=0}^T$.
This problem is know as the smoothing problem<sup>[2](#fn2)</sup>.
In the following, we estimate $\bar x(t)$, $\bar P(t)$, and $\bar P(t,t-1)$ using Kalman smoother. First, let us review Kalman filter, which is essentially a forward recursions algorithm:
\begin{align*}
 x(t|t-1)&=A x(t-1|t-1),\\
 P(t|t-1)&=AP(t-1|t-1)A^T+Q,\\
K(t)&=P(t|t-1)C^T(CP(t|t-1)C^T+R)^{-1},\\
  x(t|t)&= x(t|t-1)+K(t)(y(t)-C x(t|t-1)),\\
P(t|t)&=P(t|t-1)-K(t)CP^T(t|t-1),
\end{align*}
Then, Kalman smoother can be defined in a backward recursion manner:
\begin{align*}
J(t-1)=&P(t-1|t-1)A^TP^{-1}(t-1|t-1)\\
\bar x(t-1)=&x(t-1|t-1)+J(t-1)(\bar x(t)-Ax(t-1|t-1))\\
\bar P(t-1)=&P(t-1|t-1)+J(t-1)(\bar P(t)-P(t|t-1))J^T(t-1)\\
\bar P(t-1,t-2)=&P(t-1|t-1)J^T(t-2)\\
&+J(t-1)(\bar P(t,t-1)-AP(t-1|t-1))J^T(t-2)\\
\bar P(T,T-1)=&(I-K(t)C)AP(T-1|T-1)
\end{align*}

Now,
\begin{align*}
\log P(x(t+1),x(t)|\{y(s)\}_{s=0}^T)=&\log P(x(t)|x(t+1),\{y(s)\}_{s=0}^T)+\log P(x(t+1)|\{y(s)\}_{s=0}^T)\\
=&\log P(x(t)|x(t+1),\{y(s)\}_{s=0}^t)+\log P(x(t+1)|\{y(s)\}_{s=0}^T)\\
=&\log P(x(t+1)|x(t))+\log P(x(t)|\{y(s)\}_{s=0}^t)-\log P(x(t+1)|\{y(t)\}_{t=0}^t)\\
&+\log P(x(t+1)|\{y(s)\}_{s=0}^T)\\
=&-\frac{1}{2}(x(t+1)-A'x(t))^T{Q'}^{-1}(x(t+1)-A'x(t))\\
&-\frac{1}{2}(x(t)- x(t,t))^TP^{-1}(t|t)(x(t)- x(t,t))\\
&+\frac{1}{2}(x(t+1)- x(t+1,t))^TP^{-1}(t+1|t)(x(t+1)- x(t+1,t))\\
&-\frac{1}{2}(x(t+1)- \bar x(t+1))^T\bar P^{-1}(t+1)(x(t+1)- \bar x(t+1))+Constant,
\end{align*}
where $x(t,s)=E(x(t)|\{y(\tau)\}_{\tau=0}^s,\theta')$.

On the other hand, we have by defination that
\begin{align*}
Cov([x^T(t+1),x^T(t)]^T|\{y(t)\}_{t=0}^T)=
\begin{bmatrix}
\bar P(t+1)&\bar P(t+1,t)\\
\bar P(t,t+1)&\bar P(t)
\end{bmatrix}.
\end{align*}
Then,
\begin{align*}
\begin{bmatrix}
\bar P(t+1)&\bar P(t+1,t)\\
\bar P(t,t+1)&\bar P(t)
\end{bmatrix}
=
\begin{bmatrix}
Q^{-1}-P^{-1}(t+1|t)+\bar P^{-1}(t+1)&-Q^{-1}A\\
-A^TQ^{-1}&A^TQ^{-1}A+P^{-1}(t|t)
\end{bmatrix}^{-1}.
\end{align*}
Using matrix inverse lemma, $\bar P(t)$, $\bar P(t+1,t)$, and $\bar x(t)$ can be updated iteratively.
This is the Kalman smoother.

A different method is given in (Shumway and Stoffer, 2011).

Then, once $\{\bar P(t)\}$, $\{\bar P(t+1,t)\}$, and $\{\bar x(t)\}$ are obtained, 
\begin{align*}
Q({\theta};\theta')=&-\frac{1}{2}tr\{R^{-1}\sum_{t=0}^T[(y(t)-C\bar x(t))(y(t)-C\bar x(t))^T+C\bar P(t)C^T]\}-\frac{T+1}{2}\log |{R}|\\
&-\frac{1}{2}tr\{Q^{-1}(\Phi-\Psi {A}^T-{A}\Psi^T+{A}\Xi {A}^T)\}-\frac{T}{2}\log |{Q}|
\end{align*}



At iteration $k$:
<ul>
<li> E step: evaluating $Q(\theta;\theta_k)$ using Kalman smoother.</li>
<li> M step: $\theta_{k+1}=\arg\max_\theta Q(\theta;\theta_k)$.</li>
</ul>

</p>

<a name="fn1">[1]</a>: C. F. J. Wu, “On the convergence properties of the EM algorithm,” *The Annals of Statistics*, vol. 11, pp. 95–103, 03 1983.

<a name="fn2">[2]</a>: R. H. Shumway and D. S. Stoffer, “An approach to time series smoothing and forecasting using the em algorithm,” Journal of Time Series Analysis, vol. 3, no. 4, pp. 253–264, 1982.


