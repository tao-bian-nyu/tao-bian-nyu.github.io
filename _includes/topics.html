<div class="bottom-wide">
<h2>Dynamic Programming (DP)</h2>

<!--   For the past few decades, researchers have been puzzled by how to extend traditional VI to the continuous-time setting. </p>  -->

<div class="wrapper">
 
<div class="one">
<img src="../images/VI_algorithm.gif" alt="Continuous-time VI" style="width:600px;height:400px;">
<!-- <img src="../images/VI_algorithm.gif" alt="Continuous-time VI">  -->
</div>
<div class="two">
 <h3> Value iteration (VI) in continuous-time</h3> 
<!--  have made a breakthrough on this long-standing issue, by  -->
 <p> Originated by Bellman in 1959, VI has been widely adopted to find optimal policies for different types of systems. <br> 
However, despite its popularity, VI is flawed as it is only applicable in discrete-time. </p>
 <p> In this project, we developed a complete framework of continuous-time VI for general nonlinear optimal control problems.</p>
 
<!--   <p> VI starts from a nonnegative initial value \(V_0\). The algorithm converges to the optimal value \(V^*\) asymptotically. </p> -->

</div>
<div class="three">
  <img src="../images/PI_algorithm.jpg" alt="PI algorithm" style="width:375px;height:225px;">
</div>
<div class="four">
  <h3> Policy iteration (PI) in continuous-time</h3> 
  <p> Different from VI, PI starts from an admissible policy \(\mu_0\). Before the development of VI, PI was the dominant approach to solve continuous-time adaptive optimal control problems. </p>
  <p> The first continuous-time PI was proposed by Kleinman in 1960s. The main advantage of PI is its simple formulation and quadratic convergence speed.</p>
 </div>
</div>

 </div>
 
<div class="bottom-wide">
<h2>Reinforcement Learning (RL) and Adaptive DP (ADP)</h2>

<div class="wrapper">
<div class="five">
    <img src="../images/ADPGame.gif" alt="FlappyBird via Deep ADP" style="width:500px;height:400px;">
</div>
<div class="six">
 <div style="width:400px;overflow:auto">
 
 <p> The idea of RL, first introduced by Minsky in 1954, is to provide an understanding of activities such as the learning, memorizing, and thinking processes in human brain, and ultimately construct a system that can duplicate these “sentient”.</p> 

<p> In this research work, we developed a new RL technique, under the name of ADP, for continuous learning environments.<br>
 ADP guarantees both stability and optimality of the controlled process.</p> 
<!--  Fundamentally different from existing results, our adaptive DP method is driven by continuous-time DP algorithms, such as the continuous-time VI. -->
 
<!--  <p> Combined with deep learning, adaptive DP learns to play the standard Cartpole game provided by <a href="https://gym.openai.com/docs/">OpenAI’s gym</a>.</p> -->
  </div>
</div>
<div class="seven">
 <div style="width:400px;overflow:auto">
 <h3> Deep adaptive DP</h3> 
 <p> The Cartpole model is a widely used benchmark in control engineering. Combing ADP with deep learning techniques, we built an AI to play the Cartpole game in <a href="https://gym.openai.com/docs/">OpenAI’s gym</a>. Keras is used to build a deep neural network for approximating the unknown functions in the ADP algorithm.</p>
 </div>
 </div>
<div class="eight">
 <img src="../images/CartPole.gif" alt="CartPole via Deep ADP" style="width:280px;height:240px;">
 
</div>
</div>
</div>

 
<div class="bottom-wide">
<h2>Robust DP and robust ADP</h2>
<div class="wrapper">
<div class="one">
 <img src="../images/Portfolio.jpg" alt="Portfolio by RDP" style="width:600px;height:400px;">
</div>
<div class="two">
 <div style="width:300px;overflow:auto">
 <p> Robust DP and robust ADP aim at strengthening DP and RL algorithms so that they are robust to the presence of disturbances and adversaries in the environment. </p>
 <p> Different from existing robust optimizaton and robust learning frameworks, our methods are effective in dealing with both static and dynamic uncertainties.</p> 
  <h3> Robust DP in asset allocation</h3>
  <p> We used robust DP together with non-zero-sum game to build a learning-based dynamic asset allocation algorithm. </p>
 </div>
 </div>
<!-- <div class="ten"> 
 <p> </p>
 </div>
<div class="nigh"> 
 <p> We use robust DP and non-zero-sum differential game to build a smart dynamic portfolio allocation algorithm. </p>
 </div> -->
</div>
</div>

