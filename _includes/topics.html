<div class="bottom-wide">
<h2>Dynamic Programming (DP)</h2>

<!--   For the past few decades, researchers have been puzzled by how to extend traditional VI to the continuous-time setting. </p>  -->

<div class="wrapper">
 
<div class="one">
<img src="../images/VI_algorithm.gif" alt="Continuous-time VI" style="width:600px;height:400px;">
<!-- <img src="../images/VI_algorithm.gif" alt="Continuous-time VI">  -->
</div>
<div class="two">
 <h3> Value iteration (VI) in continuous-time</h3> 
<!--  have made a breakthrough on this long-standing issue, by  -->
 <p> Originated by Bellman in 1959, VI has been widely adopted to find optimal policies for different types of systems. <br> 
However, despite its popularity, VI is flawed as it is only applicable in discrete-time. </p>
 <p> In this research work, we developed a complete framework of continuous-time VI.</p>
 
<!--   <p> VI starts from a nonnegative initial value \(V_0\). The algorithm converges to the optimal value \(V^*\) asymptotically. </p> -->

</div>
<div class="three">
  <img src="../images/PI_algorithm.jpg" alt="PI algorithm" style="width:250px;height:150px;">
</div>
<div class="four">
  <h3> Policy iteration (PI) in continuous-time</h3> 
  <p> Different from VI, PI starts from an admissible policy \(\mu_0\). </p>
 </div>
</div>

 </div>
 
<div class="bottom-wide">
<h2>Reinforcement Learning (RL) and Adaptive DP</h2>

<div class="wrapper">
<div class="five">
    <img src="../images/ADPGame.gif" alt="FlappyBird via Deep ADP" style="width:500px;height:400px;">
</div>
<div class="six">
 <p> The idea of RL, first introduced by Minsky in 1954, is to provide an understanding of activities such as the learning, memorizing, and thinking processes in human brain, and ultimately construct a system that can duplicate these “sentient”.</p> 

<p> In this research work, we developed a new RL technique, under the name of adaptive DP, for continuous learning environments.</p> 
<!--  Fundamentally different from existing results, our adaptive DP method is driven by continuous-time DP algorithms, such as the continuous-time VI. -->
 
<!--  <p> Combined with deep learning, adaptive DP learns to play the standard Cartpole game provided by <a href="https://gym.openai.com/docs/">OpenAI’s gym</a>.</p> -->
  
</div>
<div class="seven">
 <p> Combined with deep learning, adaptive DP learns to play the standard Cartpole game provided by <a href="https://gym.openai.com/docs/">OpenAI’s gym</a>.</p>
 </div>
<div class="eight">
 <img src="../images/CartPole.gif" alt="CartPole via Deep ADP" style="width:280px;height:240px;">
 
</div>
</div>
</div>

 
<div class="bottom-wide">
<h2>Robust DP</h2>
<div class="wrapper">
<div class="one">
 <img src="../images/Portfolio.jpg" alt="Portfolio by RDP" style="width:600px;height:400px;">
</div>
<div class="two">
 <p> Robust DP aims at strengthening the DP algorithm so that it is robust to the presence of disturbance. <br>
 A remarkable feature of Robust DP is that it is effective in dealing with both static and dynamic uncertainties.</p> 
  <p> We use robust DP and non-zero-sum differential game to build a smart dynamic portfolio allocation algorithm. </p>
 </div>
<!-- <div class="ten"> 
 <p> </p>
 </div>
<div class="nigh"> 
 <p> We use robust DP and non-zero-sum differential game to build a smart dynamic portfolio allocation algorithm. </p>
 </div> -->
</div>
</div>

