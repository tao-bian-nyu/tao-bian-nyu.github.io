<div>
<h2>Dynamic Programming (DP)</h2>

<!--   For the past few decades, researchers have been puzzled by how to extend traditional VI to the continuous-time setting. </p>  -->

<div class="wrapper">
 
<div class="one">
<img src="../images/VI_algorithm.gif" alt="Continuous-time VI" style="width:600px;height:400px;">
<!-- <img src="../images/VI_algorithm.gif" alt="Continuous-time VI">  -->
</div>
<div class="two">
 <h3> Value iteration (VI) in continuous-time</h3> 
<!--  have made a breakthrough on this long-standing issue, by  -->
 <p> Originated by Bellman in 1959, VI has been widely adopted to find optimal policies for different types of systems. <br> 
However, despite its popularity, VI is flawed as it is only applicable in discrete-time. </p>
 <p> In this research work, we developed a complete framework of continuous-time VI.</p>
 
<!--   <p> VI starts from a nonnegative initial value \(V_0\). The algorithm converges to the optimal value \(V^*\) asymptotically. </p> -->

</div>
<div class="three">
  <img src="../images/PI_algorithm.pdf" alt="PI algorithm" style="width:360px;height:240px;">
</div>
<div class="four">
  <h3> Policy iteration (PI) in continuous-time</h3> 
  <p> Different from VI, PI starts from an admissible policy \(\mu_0\). </p>
 </div>
</div>

 </div>
 
 <div>
<h2>Reinforcement Learning (RL) and Adaptive DP</h2>

<div class="wrapper">
<div class="one">
    <img src="../images/ADPGame.gif" alt="FlappyBird via Deep ADP" style="width:380px;height:400px;">
</div>
<div class="two">
 <p> The idea of RL, first introduced by Minsky in 1954, is to provide an understanding of activities such as the learning, memorizing, and thinking processes in human brain, and ultimately construct a system that can duplicate these “sentient”.</p> 

<p> In this research work, we developed a new RL technique, under the name of adaptive DP, for continuous learning environments.</p> 
<!--  Fundamentally different from existing results, our adaptive DP method is driven by continuous-time DP algorithms, such as the continuous-time VI. -->
 
<!--  <p> Combined with deep learning, adaptive DP learns to play the standard Cartpole game provided by <a href="https://gym.openai.com/docs/">OpenAI’s gym</a>.</p> -->
  
</div>
<div class="three">
 <img src="../images/CartPole.gif" alt="CartPole via Deep ADP" style="width:360px;height:240px;">
 </div>
<div class="four">
 <p> Combined with deep learning, adaptive DP learns to play the standard Cartpole game provided by <a href="https://gym.openai.com/docs/">OpenAI’s gym</a>.</p>
</div>
</div>
</div>
 
 <div>
<h2>Robust DP</h2>
<div class="wrapper">
<div class="one">
 <img src="../images/Portfolio.jpg" alt="Portfolio by RDP" style="width:600px;height:400px;">
</div>
<div class="two">
 <p> 
Robust DP aims at strengthening the DP algorithm so that it is robust to the presence of disturbance. </p> 
 <p> A remarkable feature of this type of methods is that it is effective in dealing with both static and dynamic uncertainties.</p> 
 </div>
<div class="three"> 
<p> Robust DP can solve non-zero-sum differential games and robust optimal control problems. </p>
 </div>
<!-- <div class="four"> </div> -->
</div>
</div>

